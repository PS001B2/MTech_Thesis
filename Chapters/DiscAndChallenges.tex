\chapter{Discussions and Challenges}

\section{Key Findings}
Although the RMSE and \( R^2 \) score of the enhanced hybrid LSTM-RLS model were lower, the actual vs. prediction plot revealed a constant lag of one time step in the predicted stock prices. To address this issue, the DNS architecture was adopted. However, despite experimenting with various combinations of activation functions and slope trend methods, the model could not eliminate the lag.

Consequently, the DNS architecture was replaced by the ARIMA-LSTM Residual Integration Framework. Since ARIMA did not perform as expected in predicting the linear component, LSTM was utilized to predict both linear and non-linear components. However, this model also failed to remove the lag and exhibited a higher RMSE compared to other model architectures.

To further enhance performance, additional features were incorporated, and a dataset with multiple features was developed as described earlier. The Multi-Feature LSTM Forecasting Framework was then employed. Despite these enhancements, this model architecture, when combined with the proposed training method, was also unable to eliminate the lag in predictions.

Additionally, the Hybrid LSTM-RLS with a multi-feature input did not perform well. The reason behind this is that the LSTM output, which consists of daily returns, is highly volatile in nature, making it difficult for the model to generalize effectively.

The ARIMAX-LSTM-RLS model also slightly reduced performance, suggesting that the final predictions obtained from the residuals architecture do not serve as suitable inputs for the RLS model in an online prediction setting.

Furthermore, GARCH, for some reason, failed to capture any volatility (variance) in the residuals obtained after the mean predictions by ARIMAX. As a result, the final predictions produced by GARCH were identical to those obtained from ARIMAX alone, rendering GARCH ineffective in improving prediction performance.

Based on the comparative performance of different architectures tested in this dissertation, the residuals architecture has demonstrated the best predictive capability so far.

In the case of the Stochastic Volatility model, although lags were observed at certain points, the filtered volatility estimates closely followed the overall return patterns with moderate error, demonstrating the model's ability to track dynamic volatility behavior.

For the Ensemble Learning Framework, predictions from ARIMAX, Random Forest, CatBoost, LightGBM, and XGBoost were combined using an LSTM-based MetaNet. This approach successfully eliminated the one-time step lag and closely followed the actual return patterns. However, occasional directional mispredictions led to slightly elevated RMSE values. Despite this, the ensemble method outperformed all other architectures tested in this study.

The Transformer GAN-based architecture and the RAGIC model, implemented directly as described in their respective papers, showed significantly higher prediction errors. Even after extended training (up to 500 epochs), both architectures struggled to generalize. Due to limited time, these GAN-based models were not significantly modified, which might have contributed to their underperformance.

To conclude, among all the tested models and algorithms, the Ensemble Learning Framework delivered the best overall performance in terms of pattern tracking, lag elimination, and balanced error metrics.
\section{Challenges and Model Refinement}
A significant challenge faced during the development of the hybrid models was the persistent lag in long-term predictions. Despite repeated efforts to refine the models—particularly the hybrid LSTM-RLS architectures—the lag in predictions continued to appear consistently. While the introduction of alternative models aimed at mitigating this lag did result in lower RMSE values in some cases, the core issue of delayed predictions remained unresolved.

This underscores the difficulty in simultaneously achieving high accuracy and eliminating temporal lag, especially in sequential financial data. The lag persisted even with models explicitly designed for short-term responsiveness, highlighting a fundamental trade-off between complexity and real-time adaptability.

Volatility modeling through GARCH also posed substantial challenges. Despite being a well-established method, GARCH failed to capture any meaningful variance in the ARIMAX residuals. The model consistently predicted near-zero volatility, rendering it ineffective in enhancing the prediction accuracy. This unexpected outcome underlines the limitations of applying classical volatility models to highly noisy and non-linear residual structures in financial time series.

The Stochastic Volatility framework, although able to capture the broad shape and dynamics of volatility, occasionally exhibited lag and divergence from actual return magnitudes. This suggests that while it is effective in estimating underlying variance trends, aligning those estimates with return dynamics remains a complex task.

Ensemble Learning, though the most successful overall, brought its own set of challenges. Integrating predictions from multiple diverse models—including ARIMAX, Random Forest, CatBoost, LightGBM, and XGBoost—into a MetaNet required careful feature normalization, input alignment, and consistent retraining to ensure stability. Additionally, occasional directional mispredictions contributed to local spikes in error despite the overall reduction in lag.

Implementation of Transformer GAN and RAGIC architectures also presented technical and methodological difficulties. Due to time constraints, these models were implemented with minimal modifications to their original architectures. Despite extensive training, they showed high prediction errors and poor generalization. The complexity of GAN training and sensitivity to hyperparameter tuning further limited their effectiveness in this study.

In summary, while significant advancements were made in predictive modeling of stock returns, persistent challenges such as time-step lag, ineffective volatility modeling, and difficulties in training generative architectures remain unresolved. These limitations highlight areas for future research, particularly in adaptive modeling, robust volatility estimation, and customized GAN architectures for financial time series forecasting.