\chapter{Literature Review}

\section{Overview of Stock Price Prediction Methods}
Stock price prediction has been a subject of extensive research, employing both traditional statistical methods and modern machine learning techniques. Traditional approaches, such as ARIMA models, rely heavily on the detection of linear patterns in historical data and therefore are not good enough to capture complex and nonlinear dynamics in financial markets. Recent advances in machine learning have introduced neural networks, particularly Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM) networks, as promising alternatives. These models are very good at capturing temporal dependencies and nonlinear relationships, making them very suitable for time-series forecasting.

To overcome the limitations of single-model approaches, researchers have increasingly explored hybrid architectures that integrate deep learning models with advanced preprocessing techniques like Kalman Filters and Wavelet Transforms. These combinations enhance prediction accuracy by reducing noise and capturing complex patterns in financial data. A key consideration that emerges in these architectures is the tradeoff between reducing prediction lag and minimizing prediction loss. Some architectures, such as DNS and ARIMAX-LSTM residual models, focus on reducing lag to improve responsiveness, while others, including pure LSTM and GAN-based models, are optimized to reduce prediction error but often exhibit temporal lag.

\section{RNNs and LSTMs in Time-Series Forecasting}
Recurrent Neural Networks (RNNs) and their advanced variants, LSTMs, have demonstrated exceptional capabilities in time-series forecasting. LSTMs, with their ability to capture long-term dependencies, address the vanishing gradient problem that limits traditional RNNs.

For instance, \textcite{dastgerdi_investigating_2022} introduced an LSTM-based approach integrated with Kalman Filters and Wavelet Transforms for financial forecasting. Wavelet Transforms were used for signal decomposition, enhancing the signal-to-noise ratio, while the Kalman Filter refined the LSTM’s predictions by filtering out noise. This synergy improved accuracy and reliability over traditional methods.

Similarly, \textcite{fang_kalman-lstm_2021} proposed a Kalman-LSTM model for short-term traffic flow prediction, showcasing how preprocessing with Kalman Filters can improve the quality of inputs for LSTMs. These studies highlight the importance of combining noise reduction techniques with the temporal modeling capabilities of LSTMs. However, such models, although accurate, tend to introduce a slight lag in the predictions due to their dependency on prior temporal sequences.

\section{Applications of Hybrid Architectures in Time Series Forecasting}
Hybrid models combining traditional statistical methods, preprocessing filters, and deep learning networks have emerged as powerful tools for time series forecasting.

\textcite{song_improved_2022} proposed a Kalman Filter Fusing LSTM (KFFLSTM) model for tracking nonlinear dynamics, addressing challenges in environments with non-Gaussian noise. The model combined LSTMs to analyze complex nonlinear patterns, but the predictions were optimized in the light of historical information by applying Kalman filters.

In another study, \textcite{tian_application_2024} used a KF-LSTM algorithm in ultra-wideband indoor positioning systems, which proved its application in noise filtering and location estimation with robust accuracy. Similarly, \textcite{wang_hybrid_2024} combined Adaptive Extended Kalman Filters with LSTM networks for the states of charge of lithium-ion batteries, where LSTMs were used to capture the nonlinear patterns and enhance prediction accuracy.

In the stock market domain, \textcite{sahni_neoteric_2022} introduced a hybrid ARIMA-LSTM model. The ARIMA model predicted linear trends, while the LSTM captured residual nonlinearities, resulting in improved accuracy on Indian stock market data. These hybrid approaches underscore the potential of combining statistical and deep learning methods for enhanced forecasting. Notably, the ARIMA-LSTM architecture is one of the few that strikes a balance between reducing error and minimizing lag by separating trend and residual learning.

\section{GANs in Time-Series Forecasting}
Generative Adversarial Networks (GANs) have emerged as powerful tools for modeling complex data distributions, particularly in image and text generation. Recently, their application has expanded into the time-series domain to address challenges in capturing the stochastic nature of sequential data.

Traditional supervised learning models used in time-series forecasting tend to be deterministic, which limits their ability to simulate the intrinsic randomness of financial markets. In contrast, GANs generate new samples by learning to mimic the underlying distribution of the training data, offering more flexible generative capabilities.

\textcite{yoon_time-series_nodate} proposed Time-series GANs (TimeGAN), a hybrid architecture that combines adversarial training with supervised learning in a joint embedding space. TimeGAN ensures that the generated sequences preserve temporal dependencies and variable correlations by integrating both reconstruction loss and adversarial loss. This dual-objective approach significantly improves the realism and predictive power of generated sequences compared to conventional GANs.

Building on the generative capability of GANs, \textcite{gu_ragic_2025} introduced the RAGIC (Risk-Aware Generative Framework for Stock Interval Construction) architecture. Unlike typical point-prediction models, RAGIC focuses on generating realistic price intervals that account for market uncertainty and risk. Its generator learns both global and local market trends, simulating plausible future price paths infused with market randomness. By performing statistical inference on these generated sequences, RAGIC constructs dynamic prediction intervals with adaptive widths that reflect market volatility.

A key innovation in RAGIC lies in its risk-aware interval estimation, which maintains consistent coverage (95\%) while keeping the interval width narrow. This is particularly useful for decision-making in financial domains, where understanding the range of possible outcomes is often more informative than single-point forecasts.

While GAN-based methods such as TimeGAN and RAGIC significantly reduce prediction loss and better capture stochastic market behavior, they often exhibit a lag in real-time scenarios due to their generative sampling nature. Thus, they are highly suitable for long-term scenario generation and interval forecasting but may not be ideal for high-frequency prediction tasks requiring immediate responsiveness.

\section{Research Gaps and Positioning of the Study}
Despite significant advancements, several challenges remain in stock price prediction:

\begin{itemize}
    \item \textbf{Tradeoff Between Prediction Lag and Prediction Loss:} Some models focus on minimizing the time delay in capturing real-time price movements (lag), such as the Dual Network Solution (DNS) and ARIMAX-LSTM residual architectures \parencite{samanta_dual_2020}. These models are beneficial for real-time trading applications. On the other hand, architectures like LSTM, TimeGAN, and RAGIC are optimized to reduce prediction loss and capture uncertainty but may produce lagged responses.

    \item \textbf{Integration of Adaptive Filters with Neural Networks:} Most existing research focuses on using Kalman Filters and similar techniques for data preprocessing, rather than integrating them into the neural network's weight update mechanism. This presents an opportunity to explore real-time learning models that combine the adaptability of recursive filters with the pattern recognition capabilities of LSTMs.

    \item \textbf{Capturing Deep Nonlinearities:} Current hybrids (e.g., ARIMA-LSTM, Kalman-LSTM) improve over single models but still rely on relatively shallow nonlinear transformations. There is a need for architectures that explicitly learn higher-order interactions—such as attention-based layers, convolutional preprocessing, or adversarial generators—to better model the complex, nonstationary dynamics of financial time series.

    \item \textbf{Lack of Movement Prediction Metrics:} Traditional evaluation metrics like Mean Squared Error (MSE) fail to capture the effect of lag properly in real-time prediction. Novel metrics, including the Movement Prediction Metric recently proposed by \textcite{samanta_dual_2020}, provide better insights into model performance beyond error magnitude.
\end{itemize}

This dissertation aims to address these gaps by developing a novel hybrid framework that integrates recursive filtering techniques with LSTMs, focusing on reducing prediction lag while retaining the accuracy and risk-awareness advantages of deep generative models. By evaluating models not only in terms of error but also responsiveness, this study seeks to provide practical solutions for real-time financial forecasting.