\chapter{Literature Review}

\section{Overview of Stock Price Prediction Methods}
Stock price prediction has been a subject of extensive research, employing both traditional statistical methods and modern machine learning techniques. Traditional approaches, such as ARIMA models, rely heavily on the detection of linear patterns in historical data and therefore are not good enough to capture complex and nonlinear dynamics in financial markets. Recent advances in machine learning have introduced neural networks, particularly Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM) networks, as promising alternatives. These models are very good at capturing temporal dependencies and nonlinear relationships, making them very suitable for time-series forecasting.

To overcome the limitations of single-model approaches, researchers have increasingly explored hybrid architectures that integrate deep learning models with advanced preprocessing techniques like Kalman Filters and Wavelet Transforms. These combinations enhance prediction accuracy as noise and capturing complex patterns in financial data. The subsequent sections review key contributions in this domain.

\section{RNNs and LSTMs in Time-Series Forecasting}
Recurrent Neural Networks (RNNs) and their advanced variants, LSTMs, have demonstrated exceptional capabilities in time-series forecasting. LSTMs, with their ability to capture long-term dependencies, address the vanishing gradient problem that limits traditional RNNs.  

For instance, \textcite{dastgerdi_investigating_2022} introduced an LSTM-based approach integrated with Kalman Filters and Wavelet Transforms for financial forecasting. Wavelet Transforms were used for signal decomposition, enhancing the signal-to-noise ratio, while the Kalman Filter refined the LSTM’s predictions by filtering out noise. This synergy improved accuracy and reliability over traditional methods.  

Similarly, \textcite{fang_kalman-lstm_2021} proposed a Kalman-LSTM model for short-term traffic flow prediction, showcasing how preprocessing with Kalman Filters can improve the quality of inputs for LSTMs. These studies highlight the importance of combining noise reduction techniques with the temporal modeling capabilities of LSTMs.

\section{Applications of Hybrid Architectures in Time Series Forecasting}
Hybrid models combining traditional statistical methods, preprocessing filters, and deep learning networks have emerged as powerful tools for time series forecasting.  

\textcite{song_improved_2022} proposed a Kalman Filter Fusing LSTM (KFFLSTM) model for tracking nonlinear dynamics, addressing challenges in environments with non-Gaussian noise. The model combined LSTMs to analyze complex nonlinear patterns, but the predictions were optimized in the light of historical information by applying Kalman filters.  

In another study, \textcite{tian_application_2024} used a KF-LSTM algorithm in ultra-wideband indoor positioning systems, which proved its application in noise filtering and location estimation with robust accuracy. Similarly, \textcite{wang_hybrid_2024} combined Adaptive Extended Kalman Filters with LSTM networks for the states of charge of lithium-ion batteries, where LSTMs were used to capture the nonlinear patterns and enhance prediction accuracy. 

In the stock market domain, \textcite{sahni_neoteric_2022} introduced a hybrid ARIMA-LSTM model. The ARIMA model predicted linear trends, while the LSTM captured residual nonlinearities, resulting in improved accuracy on Indian stock market data. These hybrid approaches underscore the potential of combining statistical and deep learning methods for enhanced forecasting.  

\section{GANs in Time-Series Forecasting}

Generative Adversarial Networks (GANs) have emerged as powerful tools for modeling complex data distributions, particularly in image and text generation. Recently, their application has expanded into the time-series domain to address challenges in capturing the stochastic nature of sequential data.

Traditional supervised learning models used in time-series forecasting tend to be deterministic, which limits their ability to simulate the intrinsic randomness of financial markets. In contrast, GANs generate new samples by learning to mimic the underlying distribution of the training data, offering more flexible generative capabilities.

\textcite{yoon_time-series_nodate} proposed Time-series GANs (TimeGAN), a hybrid architecture that combines adversarial training with supervised learning in a joint embedding space. TimeGAN ensures that the generated sequences preserve temporal dependencies and variable correlations by integrating both reconstruction loss and adversarial loss. This dual-objective approach significantly improves the realism and predictive power of generated sequences compared to conventional GANs.

Building on the generative capability of GANs, \textcite{gu_ragic_2025} introduced the RAGIC (Risk-Aware Generative Framework for Stock Interval Construction) architecture. Unlike typical point-prediction models, RAGIC focuses on generating realistic price intervals that account for market uncertainty and risk. Its generator learns both global and local market trends, simulating plausible future price paths infused with market randomness. By performing statistical inference on these generated sequences, RAGIC constructs dynamic prediction intervals with adaptive widths that reflect market volatility.

A key innovation in RAGIC lies in its risk-aware interval estimation, which maintains consistent coverage (95\%) while keeping the interval width narrow. This is particularly useful for decision-making in financial domains, where understanding the range of possible outcomes is often more informative than single-point forecasts.

Together, these advances show that GANs, particularly architectures like TimeGAN and RAGIC, provide a promising direction for modeling complex, uncertain, and non-linear behaviors in financial time-series forecasting. They also open up new possibilities for risk-sensitive prediction frameworks that are both robust and informative.

\section{Research Gaps and Positioning of the Study}
Despite significant advancements, several challenges remain in stock price prediction:
\begin{itemize}
    \item \textbf{Lag in Predictions:} While LSTM and Kalman Filter-based models reduce overall prediction errors, they often suffer from a lag when forecasting real-time changes, particularly in volatile markets. \textcite{samanta_dual_2020} addressed this issue with a dual network solution, combining a trend predictor with a dynamic model to reduce lag. However, such approaches remain underexplored in financial forecasting.
    
    \item \textbf{Integration of Adaptive Filters with Neural Networks:} Most existing research focuses on using Kalman Filters and similar techniques for data preprocessing, rather than integrating them into the neural network's weight update mechanism. This presents an opportunity to explore real-time learning models that combine the adaptability of recursive filters with the pattern recognition capabilities of LSTMs.

    \item \textbf{Capturing Deep Nonlinearities:} Current hybrids (e.g., ARIMA-LSTM, Kalman-LSTM) improve over single models but still rely on relatively shallow nonlinear transformations. There is a need for architectures that explicitly learn higher-order interactions—such as attention-based layers, convolutional preprocessing, or adversarial generators—to better model the complex, nonstationary dynamics of financial time series.

    \item \textbf{Lack of Movement Prediction Metrics:} Traditional evaluation metrics like Mean Squared Error (MSE) fail to capture the effect of lag properly in real-time prediction. Novel metrics, including the Movement Prediction Metric recently proposed by \textcite{samanta_dual_2020} will provide better insights into model performance.
\end{itemize}

This dissertation aims to address these gaps by developing a novel hybrid framework that integrates recursive filtering techniques with LSTMs, focusing on reducing prediction lag and enhancing the accuracy of financial time-series forecasting.