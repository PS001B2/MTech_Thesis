@article{dastgerdi_investigating_2022,
	title = {Investigating the Effect of Noise Elimination on {LSTM} Models for Financial Markets Prediction Using Kalman Filter and Wavelet Transform},
	volume = {19},
	rights = {https://wseas.com/journals/bae/2022/a765107-015(2022).pdf},
	issn = {2224-2899, 1109-9526},
	url = {https://wseas.com/journals/bae/2022/a765107-015(2022).pdf},
	doi = {10.37394/23207.2022.19.39},
	abstract = {Predicting financial markets is of particular importance for investors who intend to make the most profit. Analysing reasonable and precise strategies for predicting financial markets has a long history. Deep learning techniques include analyses and predictions that can assist scientists in discovering unknown patterns of data. In this project, application of noise elimination techniques such as Wavelet transform and Kalman filter in combination of deep learning methods were discussed for predicting financial time series. The results show employing noise elimination techniques such as Wavelet transform and Kalman filter, have considerable effect on performance of {LSTM} neural network in extracting hidden patterns in the financial time series and can precisely predict future actions in these markets.},
	pages = {432--441},
	journaltitle = {{WSEAS} {TRANSACTIONS} {ON} {BUSINESS} {AND} {ECONOMICS}},
	author = {Dastgerdi, Amin Karimi and Mercorelli, Paolo},
	urldate = {2024-07-10},
	date = {2022-01-18},
	langid = {english},
	file = {Dastgerdi and Mercorelli - 2022 - Investigating the Effect of Noise Elimination on L.pdf:/Users/sharmaparth/Zotero/storage/NKBTBRRU/Dastgerdi and Mercorelli - 2022 - Investigating the Effect of Noise Elimination on L.pdf:application/pdf},
}

@article{song_improved_2022,
	title = {An Improved Kalman Filter Based on Long Short-Memory Recurrent Neural Network for Nonlinear Radar Target Tracking},
	volume = {2022},
	rights = {https://creativecommons.org/licenses/by/4.0/},
	issn = {1530-8677, 1530-8669},
	url = {https://www.hindawi.com/journals/wcmc/2022/8280428/},
	doi = {10.1155/2022/8280428},
	abstract = {The target tracking of nonlinear maneuvering radar in dense clutter environments is still an important but difficult problem to be solved effectively. Traditional solutions often rely on motion models and prior distributions. This paper presents a novel improved architecture of Kalman filter based on a recursive neural network, which combines the sequence learning of recurrent neural networks with the precise prediction of Kalman filter in an end-to-end manner. We employ three {LSTM} networks to model nonlinear motion equation, motion noise, and measurement noise, respectively, and learn their long-term dependence from a large amount of training data. They are then applied to the prediction and update process of Kalman filter to calculate the estimated target state. Our approach is able to address the tracking problem of nonlinear maneuvering radar target online end-to-end and does not require the motion models and prior distributions. Experimental results show that our method is more effective and faster than the traditional methods and more accurate than the method with {LSTM} network alone.},
	pages = {1--10},
	journaltitle = {Wireless Communications and Mobile Computing},
	shortjournal = {Wireless Communications and Mobile Computing},
	author = {Song, Fei and Li, Yong and Cheng, Wei and Dong, Limeng and Li, Minqi and Li, Junfang},
	editor = {Alamoodi, A.H.},
	urldate = {2024-06-11},
	date = {2022-08-21},
	langid = {english},
	file = {Song et al. - 2022 - An Improved Kalman Filter Based on Long Short-Memo.pdf:/Users/sharmaparth/Zotero/storage/Q3973LNR/Song et al. - 2022 - An Improved Kalman Filter Based on Long Short-Memo.pdf:application/pdf},
}

@misc{wang_hybrid_2024,
	title = {A Hybrid Model for State of Charge Estimation of Lithium-Ion Batteries Utilizing Improved Adaptive Extended Kalman Filter and Long Short-Term Memory Neural Network},
	url = {https://www.ssrn.com/abstract=4834979},
	doi = {10.2139/ssrn.4834979},
	author = {Wang, Chunsheng and Li, Ripeng and Cao, Yuan and Li, Mutian},
	urldate = {2024-07-05},
	date = {2024},
	langid = {english},
	file = {Wang et al. - 2024 - A Hybrid Model for State of Charge Estimation of L.pdf:/Users/sharmaparth/Zotero/storage/7I78P7Z5/Wang et al. - 2024 - A Hybrid Model for State of Charge Estimation of L.pdf:application/pdf},
}

@inproceedings{fang_kalman-lstm_2021,
	location = {Chongqing, China},
	title = {Kalman-{LSTM} Model for Short-term Traffic Flow Forecasting},
	rights = {https://doi.org/10.15223/policy-029},
	isbn = {978-1-72818-028-1},
	url = {https://ieeexplore.ieee.org/document/9390991/},
	doi = {10.1109/IAEAC50856.2021.9390991},
	abstract = {This paper proposes a time prediction model based on Kalman filtering and {LSTM}, namely the Kalman {LSTM} model, which is used to predict time series data with long-term and short-term characteristics. The {KalmanLSTM} model uses {LSTM}'s unique storage function to "store" the information contained in the pre-order data. It is then used to obtain the basic time series of the prediction problem in subsequent processing. The following Kalman filter model dynamically adjusts the basic time data series obtained by {LSTM} processing. Finally, we will obtain adjusted forecasts. Here, we establish training and test set samples to train the Kalman-{LSTM} model and test the performance of the sample model. For comparison, we evaluated the {RMSE} (root mean square error) indicator of the conventional {LSTM} model with the Kalman-{LSTM} model. The results show that the Kalman-{LSTM} model is superior to the {LSTM} model, and can get more accurate predictions.},
	eventtitle = {2021 {IEEE} 5th Advanced Information Technology, Electronic and Automation Control Conference ({IAEAC})},
	pages = {1604--1608},
	booktitle = {2021 {IEEE} 5th Advanced Information Technology, Electronic and Automation Control Conference ({IAEAC})},
	publisher = {{IEEE}},
	author = {Fang, Weiwei and Cai, Weihong and Fan, Bo and Yan, Jingwen and Zhou, Teng},
	urldate = {2024-06-11},
	date = {2021-03-12},
	langid = {english},
	file = {Fang et al. - 2021 - Kalman-LSTM Model for Short-term Traffic Flow Fore.pdf:/Users/sharmaparth/Zotero/storage/CQN4PUKS/Fang et al. - 2021 - Kalman-LSTM Model for Short-term Traffic Flow Fore.pdf:application/pdf},
}

@article{tian_application_2024,
	title = {Application of a long short-term memory neural network algorithm fused with Kalman filter in {UWB} indoor positioning},
	volume = {14},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-024-52464-y},
	doi = {10.1038/s41598-024-52464-y},
	abstract = {Abstract
            Ultra-wideband technology has good anti-interference capabilities and development prospects in indoor positioning. Since ultra-wideband will be affected by random errors in indoor positioning, to exploit the advantages of the Kalman filter ({KF}) and the long short-term memory ({LSTM}) network, this paper proposes a long short-term memory neural network algorithm fused with the Kalman filter ({KF}–{LSTM}) to improve {UWB} positioning. First, the ultra-wideband data is processed through {KF} to weaken the noise in the data, and then the data is fed into the {LSTM} network for training, and the capability of the {LSTM} network to process time series features is employed to obtain more accurate label positions. Finally, simulation and measurement results show that the {KF}–{LSTM} algorithm achieves 71.31\%, 37.28\%, and 49.31\% higher average positioning accuracy than the back propagation ({BP}) network, (back propagation network fused with the Kalman filter ({KF}-{BP}), and {LSTM} network algorithms, respectively, and the {KF}–{LSTM} algorithm performs more stably. Meanwhile, the more noise the data contains, the more obvious the stability contrast between the four algorithms.},
	pages = {1925},
	number = {1},
	journaltitle = {Scientific Reports},
	shortjournal = {Sci Rep},
	author = {Tian, Yalin and Lian, Zengzeng and Wang, Penghui and Wang, Mengqi and Yue, Zhe and Chai, Huabin},
	urldate = {2024-06-11},
	date = {2024-01-22},
	langid = {english},
	file = {Tian et al. - 2024 - Application of a long short-term memory neural net.pdf:/Users/sharmaparth/Zotero/storage/EZL4YTR5/Tian et al. - 2024 - Application of a long short-term memory neural net.pdf:application/pdf},
}

@inproceedings{samanta_dual_2020,
	location = {Glasgow, United Kingdom},
	title = {A Dual Network Solution ({DNS}) for Lag-Free Time Series Forecasting},
	rights = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/{IEEE}.html},
	isbn = {978-1-72816-926-2},
	url = {https://ieeexplore.ieee.org/document/9207022/},
	doi = {10.1109/IJCNN48605.2020.9207022},
	abstract = {When it comes to time series forecasting, lag in the predicted sequence can be a predominant issue. Unfortunately, this is often overlooked in most of the time series literature as this does not contribute to a high prediction error (i.e. {MSE}). However, it leads to a rather poor forecast in terms of movement prediction in time series. In this article, we tackle this basic problem with a novel trend driven mechanism. Trend, deﬁned as the inherent pattern of the data, is extracted here and utilized next to perform a lag-free forecasting. We propose a generic and light Dual Network Solution ({DNS}), where the ﬁrst network predicts the trend and the second network utilizes that predicted trend along with its historical information to capture the dynamical behavior of the time series efﬁciently. {DNS} exhibits a substantially improved (≈ 10\% better) performance compared to more complex and resource-intensive state-of-theart algorithms in large scale regression problems. Apart from the traditional Mean Squared Error ({MSE}), we also propose a new Movement Prediction Metric or {MPM} (for detection of lag in time series) as a new complementary performance metric to evaluate the efﬁcacy of {DNS} better.},
	eventtitle = {2020 International Joint Conference on Neural Networks ({IJCNN})},
	pages = {1--8},
	booktitle = {2020 International Joint Conference on Neural Networks ({IJCNN})},
	publisher = {{IEEE}},
	author = {Samanta, Subhrajit and Pratama, Mahardhika and Sundaram, Suresh and Srikanth, Narasimalu},
	urldate = {2024-08-05},
	date = {2020-07},
	langid = {english},
	file = {Samanta et al. - 2020 - A Dual Network Solution (DNS) for Lag-Free Time Se.pdf:/Users/sharmaparth/Zotero/storage/QSGFIIE4/Samanta et al. - 2020 - A Dual Network Solution (DNS) for Lag-Free Time Se.pdf:application/pdf},
}

@incollection{sahni_neoteric_2022,
	location = {Singapore},
	title = {A Neoteric Technique Using {ARIMA}-{LSTM} for Time Series Analysis on Stock Market Forecasting},
	volume = {1405},
	isbn = {9789811659515 9789811659522},
	url = {https://link.springer.com/10.1007/978-981-16-5952-2_33},
	abstract = {Stock market time-series analysis and its price prediction have been intriguing the human mind ever since they were in existence. Analysis of time series with the help of various models has become imperative not only in business but also in every day stock market practices as algorithms improve the speed, accuracy, and discipline and one can always backtest these strategies to see the type of performance achieved in a real-time environment. Numerous models have been put forward to improve the accuracy and robustness in forecasting market prices. However, due to the high volatility and non-stationary nature of the stock market data, many of these models have been ineffective in forecasting future trends. Addition of artificial neural networks to such models would lead to giant leaps in this domain providing reliable and accurate models as the weights are assigned to the input parameters, are computed in the training phase and are adapted by learning with gradient descent and backpropagation algorithm. This paper proposes an aggregation of the novel autoregressive integrated moving average ({ARIMA}) model with the long short- term memory ({LSTM}) model to increase the forecasting accuracy by using the dynamism of neural networks, thereby generating the best-fitted coefficients for the model. The methodology of this model is then evaluated by comparing the empirical results with other conventional models on the Indian stock market data. Accordingly, it is observed that this model achieves significantly better forecasting accuracy and can alternatively be put forward to be used as a suitable model.},
	pages = {381--392},
	booktitle = {Mathematical Modeling, Computational Intelligence Techniques and Renewable Energy},
	publisher = {Springer Singapore},
	author = {Shah, Hetvi and Bhatt, Vishva and Shah, Jigarkumar},
	editor = {Sahni, Manoj and Merigó, José M. and Sahni, Ritu and Verma, Rajkumar},
	urldate = {2024-07-31},
	date = {2022},
	langid = {english},
	doi = {10.1007/978-981-16-5952-2_33},
	note = {Series Title: Advances in Intelligent Systems and Computing},
	file = {Shah et al. - 2022 - A Neoteric Technique Using ARIMA-LSTM for Time Ser.pdf:/Users/sharmaparth/Zotero/storage/SWQFQJZE/Shah et al. - 2022 - A Neoteric Technique Using ARIMA-LSTM for Time Ser.pdf:application/pdf},
}


@article{yoon_time-series_nodate,
	title = {Time-series Generative Adversarial Networks},
	abstract = {A good generative model for time-series data should preserve temporal dynamics, in the sense that new sequences respect the original relationships between variables across time. Existing methods that bring generative adversarial networks ({GANs}) into the sequential setting do not adequately attend to the temporal correlations unique to time-series data. At the same time, supervised models for sequence prediction—which allow ﬁner control over network dynamics—are inherently deterministic. We propose a novel framework for generating realistic time-series data that combines the ﬂexibility of the unsupervised paradigm with the control afforded by supervised training. Through a learned embedding space jointly optimized with both supervised and adversarial objectives, we encourage the network to adhere to the dynamics of the training data during sampling. Empirically, we evaluate the ability of our method to generate realistic samples using a variety of real and synthetic time-series datasets. Qualitatively and quantitatively, we ﬁnd that the proposed framework consistently and signiﬁcantly outperforms state-of-the-art benchmarks with respect to measures of similarity and predictive ability.},
	author = {Yoon, Jinsung and Jarrett, Daniel},
	langid = {english},
	file = {Yoon and Jarrett - Time-series Generative Adversarial Networks.pdf:/Users/sharmaparth/Zotero/storage/HEEXX4IG/Yoon and Jarrett - Time-series Generative Adversarial Networks.pdf:application/pdf},
}


@article{gu_ragic_2025,
	title = {{RAGIC}: Risk-Aware Generative Framework for Stock Interval Construction},
	volume = {37},
	rights = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/{IEEE}.html},
	issn = {1041-4347, 1558-2191, 2326-3865},
	url = {https://ieeexplore.ieee.org/document/10885039/},
	doi = {10.1109/TKDE.2025.3533492},
	shorttitle = {{RAGIC}},
	abstract = {Efforts to predict stock market outcomes have yielded limited success due to the inherently stochastic nature of the market, inﬂuenced by numerous unpredictable factors. Many existing prediction approaches focus on single-point predictions, lacking the depth needed for effective decision-making and often overlooking market risk. To bridge this gap, we propose {RAGIC}, a novel riskaware framework for stock interval prediction to quantify uncertainty. Our approach leverages a Generative Adversarial Network ({GAN}) to produce future price sequences infused with randomness inherent in ﬁnancial markets. {RAGIC}’s generator detects the risk perception of informed investors and captures historical price trends globally and locally. Then the risk-sensitive intervals is built upon the simulated future prices from sequence generation through statistical inference, incorporating horizon-wise insights. The interval’s width is adaptively adjusted to reﬂect market volatility. Importantly, our approach relies solely on publicly available data and incurs only low computational overhead. {RAGIC}’s evaluation across globally recognized broad-based indices demonstrates its balanced performance, offering both accuracy and informativeness. Achieving a consistent 95\% coverage, {RAGIC} maintains a narrow interval width. This promising outcome suggests that our approach effectively addresses the challenges of stock market prediction while incorporating vital risk considerations.},
	pages = {2085--2096},
	number = {4},
	journaltitle = {{IEEE} Transactions on Knowledge and Data Engineering},
	shortjournal = {{IEEE} Trans. Knowl. Data Eng.},
	author = {Gu, Jingyi and Du, Wenlu and Wang, Guiling},
	urldate = {2025-04-11},
	date = {2025-04},
	langid = {english},
	file = {Gu et al. - 2025 - RAGIC Risk-Aware Generative Framework for Stock I.pdf:/Users/sharmaparth/Zotero/storage/E469YKD8/Gu et al. - 2025 - RAGIC Risk-Aware Generative Framework for Stock I.pdf:application/pdf},
}


@article{sherstinsky_fundamentals_2020,
	title = {Fundamentals of Recurrent Neural Network ({RNN}) and Long Short-Term Memory ({LSTM}) network},
	volume = {404},
	issn = {01672789},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167278919305974},
	doi = {10.1016/j.physd.2019.132306},
	abstract = {Because of their effectiveness in broad practical applications, {LSTM} networks have received a wealth of coverage in scientific journals, technical blogs, and implementation guides. However, in most articles, the inference formulas for the {LSTM} network and its parent, {RNN}, are stated axiomatically, while the training formulas are omitted altogether. In addition, the technique of ‘‘unrolling’’ an {RNN} is routinely presented without justification throughout the literature. The goal of this tutorial is to explain the essential {RNN} and {LSTM} fundamentals in a single document. Drawing from concepts in Signal Processing, we formally derive the canonical {RNN} formulation from differential equations. We then propose and prove a precise statement, which yields the {RNN} unrolling technique. We also review the difficulties with training the standard {RNN} and address them by transforming the {RNN} into the ‘‘Vanilla {LSTM}’’1 network through a series of logical arguments. We provide all equations pertaining to the {LSTM} system together with detailed descriptions of its constituent entities. Albeit unconventional, our choice of notation and the method for presenting the {LSTM} system emphasizes ease of understanding. As part of the analysis, we identify new opportunities to enrich the {LSTM} system and incorporate these extensions into the Vanilla {LSTM} network, producing the most general {LSTM} variant to date. The target reader has already been exposed to {RNNs} and {LSTM} networks through numerous available resources and is open to an alternative pedagogical approach. A Machine Learning practitioner seeking guidance for implementing our new augmented {LSTM} model in software for experimentation and research will find the insights and derivations in this treatise valuable as well.},
	pages = {132306},
	journaltitle = {Physica D: Nonlinear Phenomena},
	shortjournal = {Physica D: Nonlinear Phenomena},
	author = {Sherstinsky, Alex},
	urldate = {2024-10-15},
	date = {2020-03},
	langid = {english},
	file = {Sherstinsky - 2020 - Fundamentals of Recurrent Neural Network (RNN) and.pdf:/Users/sharmaparth/Zotero/storage/HUGHQVUJ/Sherstinsky - 2020 - Fundamentals of Recurrent Neural Network (RNN) and.pdf:application/pdf},
}


@article{hochreiter_long_1997,
	title = {Long Short-Term Memory},
	volume = {9},
	issn = {0899-7667},
	url = {https://doi.org/10.1162/neco.1997.9.8.1735},
	doi = {10.1162/neco.1997.9.8.1735},
	abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory ({LSTM}). Truncating the gradient where this does not do harm, {LSTM} can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. {LSTM} is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, {LSTM} leads to many more successful runs, and learns much faster. {LSTM} also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
	pages = {1735--1780},
	number = {8},
	journaltitle = {Neural Computation},
	author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
	date = {1997-11},
	note = {\_eprint: https://direct.mit.edu/neco/article-pdf/9/8/1735/813796/neco.1997.9.8.1735.pdf},
}



@online{nse_vix,
  author    = {{National Stock Exchange of India}},
  title     = {Historical Volatility Index (VIX) Reports},
  year      = {2024},
  url       = {https://www.nseindia.com/reports-indices-historical-vix},
  note      = {Accessed: May 16, 2025}
}

@online{yahoo_nifty50_history,
  author    = {{Yahoo Finance}},
  title     = {{NIFTY 50 (\textasciicircum NSEI) Historical Data}},
  year      = {2025},
  url       = {https://finance.yahoo.com/quote/%5ENSEI/history/},
  note      = {Accessed: May 20, 2025}
}